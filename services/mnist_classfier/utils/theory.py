import numpy as np
from sklearn.datasets import fetch_openml
import streamlit as st
import matplotlib.pyplot as plt
import pickle


def visualize_mnist(X, y):
    unique_labels = np.unique(y)
    images = []

    # L·∫•y m·ªôt ·∫£nh cho m·ªói nh√£n t·ª´ 0 ƒë·∫øn 9
    for label in unique_labels:
        idx = np.nonzero(y == label)[0][0]  # L·∫•y index ƒë·∫ßu ti√™n c·ªßa label
        images.append((X[idx], label))

    fig, axes = plt.subplots(2, 5, figsize=(7, 3))
    # fig.suptitle("Nh·ªØng nh√£n trong t·∫≠p d·ªØ li·ªáu MNIST", fontsize=10)

    for i, ax in enumerate(axes.flat):
        ax.imshow(images[i][0].reshape(28, 28), cmap="gray")
        ax.set_title(f"Label: {images[i][1]}")
        ax.axis("off")

    st.pyplot(fig)


# @st.cache_data
@st.cache_data
def mnist_dataset():

    st.markdown("## üìú T·∫≠p d·ªØ li·ªáu MNIST")
    st.write("---")

    with open('./services/mnist_classfier/data/X.pkl', 'rb') as f:
        X = pickle.load(f)

    with open('./services/mnist_classfier/data/y.pkl', 'rb') as f:
        y = pickle.load(f)

    col1, col2 = st.columns(2)
    with col1:
        visualize_mnist(X, y)
        st.write(
            f"T·∫≠p d·ªØ li·ªáu MNIST g·ªìm {X.shape[0]} m·∫´u, {X.shape[1]} ƒë·∫∑c tr∆∞ng")

    with col2:
        st.markdown("""
            **MNIST (Modified National Institute of Standards and Technology)** l√† m·ªôt trong
            nh·ªØng t·∫≠p d·ªØ li·ªáu ph·ªï bi·∫øn nh·∫•t trong lƒ©nh v·ª±c nh·∫≠n d·∫°ng ch·ªØ s·ªë vi·∫øt tay. ƒê√¢y
            l√† m·ªôt t·∫≠p d·ªØ li·ªáu ti√™u chu·∫©n ƒë·ªÉ hu·∫•n luy·ªán v√† ƒë√°nh gi√° c√°c m√¥ h√¨nh machine
            learning (ML) v√† deep learning (DL), ƒë·∫∑c bi·ªát l√† c√°c m√¥ h√¨nh nh·∫≠n d·∫°ng h√¨nh ·∫£nh.

            MNIST g·ªìm 70.000 ·∫£nh ch·ªØ s·ªë vi·∫øt tay v·ªõi k√≠ch th∆∞·ªõc ·∫£nh 28x28 pixel, ·∫£nh grayscale(ƒëen tr·∫Øng, 1 k√™nh m√†u)
            - 60.000 ·∫£nh d√πng ƒë·ªÉ hu·∫•n luy·ªán (training set)
            - 10.000 ·∫£nh d√πng ƒë·ªÉ ƒë√°nh gi√° (test set)

            - S·ªë l·ªõp (s·ªë nh√£n): 10 (c√°c ch·ªØ s·ªë t·ª´ 0 ƒë·∫øn 9)

            M·ªói ·∫£nh ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng ma tr·∫≠n 28x28

            Ta s·∫Ω chu·∫©n h√≥a d·ªØ li·ªáu, ƒë∆∞a gi√° tr·ªã pixel ban ƒë·∫ßu n·∫±m trong kho·∫£ng [0, 255],
            c·∫ßn chia cho 255.0 ƒë·ªÉ ƒë∆∞a v·ªÅ kho·∫£ng [0,1]
        """)

    # Visualize target distribution
    fig, ax = plt.subplots(figsize=(7, 3))
    unique, counts = np.unique(y, return_counts=True)
    ax.bar(unique, counts, tick_label=unique)
    ax.set_title("Ph√¢n ph·ªëi c√°c nh√£n trong t·∫≠p d·ªØ li·ªáu MNIST")
    ax.set_xlabel("Nh√£n")
    ax.set_ylabel("S·ªë l∆∞·ª£ng")
    st.pyplot(fig)

    st.write("---")

    return X, y


def decision_tree_theory():
    # Ti√™u ƒë·ªÅ ch√≠nh
    st.header("Thu·∫≠t to√°n Decision Tree (C√¢y Quy·∫øt ƒê·ªãnh)")
    st.write("""
    - **Decision Tree** l√† m·ªôt thu·∫≠t to√°n h·ªçc m√°y d√πng cho c·∫£ ph√¢n lo·∫°i v√† h·ªìi quy. N√≥ chia d·ªØ li·ªáu th√†nh c√°c v√πng d·ª±a tr√™n c√°c ƒëi·ªÅu ki·ªán quy·∫øt ƒë·ªãnh, ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng c·∫•u tr√∫c c√¢y v·ªõi n√∫t g·ªëc, n√∫t n·ªôi b·ªô, v√† n√∫t l√°.

    - **·ª®ng d·ª•ng**: Ph√¢n lo·∫°i email, d·ª± ƒëo√°n gi√° nh√†, ch·∫©n ƒëo√°n y khoa.
    """)

    col1, col2 = st.columns(2)
    with col1:

        # Ph·∫ßn 1: C√°c kh√°i ni·ªám c∆° b·∫£n
        st.subheader("C√°c kh√°i ni·ªám c∆° b·∫£n")
        st.write("""
        - **ƒê·∫∑c tr∆∞ng (Feature)**: C√°c bi·∫øn ƒë·∫ßu v√†o (v√≠ d·ª•: tu·ªïi, thu nh·∫≠p).
        - **Nh√£n (Label)**: Bi·∫øn ƒë·∫ßu ra c·∫ßn d·ª± ƒëo√°n (v√≠ d·ª•: C√≥/Kh√¥ng).
        - **ƒê·ªô kh√¥ng thu·∫ßn khi·∫øt (Impurity)**: M·ª©c ƒë·ªô h·ªón t·∫°p c·ªßa d·ªØ li·ªáu trong m·ªôt n√∫t.
        - **Chia nh√°nh (Splitting)**: Qu√° tr√¨nh chia d·ªØ li·ªáu d·ª±a tr√™n m·ªôt ƒëi·ªÅu ki·ªán.
        """)

        # Ph·∫ßn 2: C√°ch ho·∫°t ƒë·ªông
        st.subheader("C√°ch ho·∫°t ƒë·ªông c·ªßa Decision Tree")
        st.write("""
        1. **Ch·ªçn ƒë·∫∑c tr∆∞ng v√† ng∆∞·ª°ng t·ªët nh·∫•t**:
            - Duy·ªát qua c√°c ƒë·∫∑c tr∆∞ng v√† gi√° tr·ªã ƒë·ªÉ t√¨m c√°ch chia gi·∫£m ƒë·ªô kh√¥ng thu·∫ßn khi·∫øt nhi·ªÅu nh·∫•t.
        2. **Chia d·ªØ li·ªáu**:
            - D·ªØ li·ªáu ƒë∆∞·ª£c chia th√†nh c√°c nh√°nh d·ª±a tr√™n ƒëi·ªÅu ki·ªán (v√≠ d·ª•: "tu·ªïi ‚â§ 30").
        3. **L·∫∑p l·∫°i**:
            - Ti·∫øp t·ª•c chia tr√™n m·ªói nh√°nh cho ƒë·∫øn khi ƒë·∫°t ƒëi·ªÅu ki·ªán d·ª´ng (thu·∫ßn khi·∫øt, ƒë·ªô s√¢u t·ªëi ƒëa, s·ªë m·∫´u t·ªëi thi·ªÉu).
        4. **G√°n gi√° tr·ªã n√∫t l√°**:
            - Ph√¢n lo·∫°i: Ch·ªçn nh√£n ph·ªï bi·∫øn nh·∫•t.
            - H·ªìi quy: Ch·ªçn gi√° tr·ªã trung b√¨nh.
        """)

    with col2:

        # Ph·∫ßn 3: C√¥ng th·ª©c to√°n h·ªçc
        st.subheader("C√¥ng th·ª©c to√°n h·ªçc")

        st.markdown("""
        #### Gini Index (Ph√¢n lo·∫°i)
        - **ƒêo ƒë·ªô kh√¥ng thu·∫ßn khi·∫øt:** """)

        st.latex(r"\text{Gini} = 1 - \sum_{i=1}^{c} p_i^2")
        st.write("""
        - $p_i$: T·ª∑ l·ªá m·∫´u thu·ªôc l·ªõp i.

        - $c$: S·ªë l·ªõp.
        """)

        st.write("""
        #### Entropy v√† Information Gain (Ph√¢n lo·∫°i)

        - **Entropy**:
        """)

        st.latex(r"\text{Entropy} = - \sum_{i=1}^{c} p_i \log_2(p_i)")

        st.write("- **Information Gain**:")

        st.latex(
            r"\text{IG} = \text{Entropy(parent)} - \sum_{j} \frac{N_j}{N} \text{Entropy(child}_j\text{)}")

        # st.write("#### Mean Squared Error (H·ªìi quy)")

        # st.latex(r"\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \bar{y})^2")

        # st.markdown(r"""
        # - $y_i$: Gi√° tr·ªã th·ª±c t·∫ø.

        # - $\bar{y}$: Gi√° tr·ªã trung b√¨nh.

        """)

    # # Ph·∫ßn 4: V√≠ d·ª• minh h·ªça
    # st.subheader("V√≠ d·ª• minh h·ªça")
    # st.write("""
    # Gi·∫£ s·ª≠ d·ªØ li·ªáu c√≥ 2 ƒë·∫∑c tr∆∞ng: "Tu·ªïi" v√† "Thu nh·∫≠p", nh√£n l√† "Mua nh√†" (C√≥/Kh√¥ng):
    # - B∆∞·ªõc 1: T√≠nh Gini cho to√†n b·ªô d·ªØ li·ªáu.
    # - B∆∞·ªõc 2: Chia v·ªõi "Tu·ªïi ‚â§ 30":
    #     - Nh√°nh ‚â§ 30: 80% Kh√¥ng, 20% C√≥.
    #     - Nh√°nh > 30: 60% C√≥, 40% Kh√¥ng.
    # - B∆∞·ªõc 3: Ch·ªçn "Tu·ªïi ‚â§ 30" l√†m n√∫t g·ªëc n·∫øu Gini gi·∫£m nhi·ªÅu.
    # - B∆∞·ªõc 4: Ti·∫øp t·ª•c chia tr√™n nh√°nh v·ªõi "Thu nh·∫≠p".
    # """)

    with col1:
        # Ph·∫ßn 5: ∆Øu v√† nh∆∞·ª£c ƒëi·ªÉm
        st.subheader("∆Øu v√† nh∆∞·ª£c ƒëi·ªÉm")
        st.write("""
        **∆Øu ƒëi·ªÉm**:
        - D·ªÖ hi·ªÉu, tr·ª±c quan.
        - Kh√¥ng c·∫ßn chu·∫©n h√≥a d·ªØ li·ªáu.
        - X·ª≠ l√Ω ƒë∆∞·ª£c c·∫£ d·ªØ li·ªáu s·ªë v√† ph√¢n lo·∫°i.

        **Nh∆∞·ª£c ƒëi·ªÉm**:
        - D·ªÖ b·ªã overfitting n·∫øu kh√¥ng gi·ªõi h·∫°n ƒë·ªô s√¢u.
        - Nh·∫°y c·∫£m v·ªõi nhi·ªÖu v√† d·ªØ li·ªáu m·∫•t c√¢n b·∫±ng.
        - Kh√¥ng t·ªët v·ªõi m·ªëi quan h·ªá phi tuy·∫øn ph·ª©c t·∫°p.
        """)


def svm_theory():
    st.markdown(r"""
    ## Support Vector Machine - SVM

    ### 1. T·ªïng quan

    **M√°y Vector H·ªó tr·ª£ (SVM)** l√† m·ªôt thu·∫≠t to√°n h·ªçc m√°y c√≥ gi√°m s√°t, ƒë∆∞·ª£c s·ª≠ d·ª•ng ch·ªß y·∫øu cho c√°c b√†i to√°n ph√¢n lo·∫°i (classification) v√† c√≥ th·ªÉ m·ªü r·ªông cho h·ªìi quy (regression). SVM ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi Vladimir Vapnik v√† c√°c c·ªông s·ª± v√†o nh·ªØng nƒÉm 1990. √ù t∆∞·ªüng ch√≠nh c·ªßa SVM l√† t√¨m m·ªôt **si√™u ph·∫≥ng (hyperplane)** ph√¢n t√°ch t·ªët nh·∫•t gi·ªØa c√°c l·ªõp d·ªØ li·ªáu, sao cho kho·∫£ng c√°ch t·ª´ si√™u ph·∫≥ng ƒë·∫øn c√°c ƒëi·ªÉm d·ªØ li·ªáu g·∫ßn nh·∫•t c·ªßa m·ªói l·ªõp (g·ªçi l√† **support vectors**) l√† l·ªõn nh·∫•t.

    M·ªôt s·ªë kh√°i ni·ªám c∆° b·∫£n trong SVM:

    - **Si√™u ph·∫≥ng**: Trong kh√¥ng gian $n$-chi·ªÅu, si√™u ph·∫≥ng ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a b·ªüi ph∆∞∆°ng tr√¨nh """)

    st.latex(r"w^T x + b = 0 ")

    st.markdown(r"""
    Trong ƒë√≥:

    $w$ l√† vector ph√°p tuy·∫øn

    $x$ l√† ƒëi·ªÉm trong kh√¥ng gian

    $b$ l√† h·ªá s·ªë t·ª± do.

    - V√≠ d·ª•: Trong kh√¥ng gian 2 chi·ªÅu, si√™u ph·∫≥ng l√† m·ªôt ƒë∆∞·ªùng th·∫≥ng; trong kh√¥ng gian 3 chi·ªÅu, si√™u ph·∫≥ng l√† m·ªôt m·∫∑t ph·∫≥ng.
    - **Support Vectors**: C√°c ƒëi·ªÉm d·ªØ li·ªáu n·∫±m g·∫ßn nh·∫•t v·ªõi si√™u ph·∫≥ng ph√¢n t√°ch, ƒë√≥ng vai tr√≤ quan tr·ªçng trong vi·ªác x√°c ƒë·ªãnh v·ªã tr√≠ v√† h∆∞·ªõng c·ªßa si√™u ph·∫≥ng.
    - **Margin**: Kho·∫£ng c√°ch t·ª´ si√™u ph·∫≥ng ƒë·∫øn support vectors. SVM t√¨m si√™u ph·∫≥ng sao cho margin n√†y l√† l·ªõn nh·∫•t.

    SVM c√≥ th·ªÉ x·ª≠ l√Ω c·∫£ d·ªØ li·ªáu **tuy·∫øn t√≠nh ph√¢n t√°ch** v√† **kh√¥ng tuy·∫øn t√≠nh ph√¢n t√°ch** nh·ªù s·ª≠ d·ª•ng **kernel trick**, gi√∫p √°nh x·∫° d·ªØ li·ªáu l√™n kh√¥ng gian chi·ªÅu cao h∆°n ƒë·ªÉ ph√¢n t√°ch d·ªÖ d√†ng h∆°n.

    ---

    ### 2. Nguy√™n l√Ω ho·∫°t ƒë·ªông

    SVM ho·∫°t ƒë·ªông d·ª±a tr√™n vi·ªác t·ªëi ∆∞u h√≥a si√™u ph·∫≥ng ph√¢n t√°ch gi·ªØa c√°c l·ªõp d·ªØ li·ªáu. C√≥ hai tr∆∞·ªùng h·ª£p ch√≠nh: d·ªØ li·ªáu **tuy·∫øn t√≠nh ph√¢n t√°ch** v√† **kh√¥ng tuy·∫øn t√≠nh ph√¢n t√°ch**.

    #### 2.1. Tr∆∞·ªùng h·ª£p tuy·∫øn t√≠nh ph√¢n t√°ch (Hard Margin SVM)

    **M·ª•c ti√™u**: T√¨m si√™u ph·∫≥ng ph√¢n t√°ch sao cho **margin** gi·ªØa c√°c l·ªõp l√† l·ªõn nh·∫•t.

    Gi·∫£ s·ª≠ t·∫≠p d·ªØ li·ªáu $(x_i, y_i)_{i=1}^N$, v·ªõi $x_i \in \mathbb{R}^n$ l√† vector ƒë·∫∑c tr∆∞ng v√† $y_i \in \{-1, 1\}$ l√† nh√£n l·ªõp.
    Si√™u ph·∫≥ng ph√¢n t√°ch ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·ªüi:""")

    st.latex(r"w^T x + b = 0")

    st.markdown(r"ƒê·ªÉ ph√¢n t√°ch ƒë√∫ng, c√°c ƒëi·ªÉm d·ªØ li·ªáu ph·∫£i th·ªèa m√£n:")

    st.latex(r"y_i (w^T x_i + b) \geq 1 \quad \forall i")

    st.markdown(r"**Margin** ƒë∆∞·ª£c t√≠nh b·∫±ng:")

    st.latex(r"\text{Margin} = \frac{2}{\|w\|}")

    st.markdown(
        r"SVM t√¨m $w$ v√† $b$ ƒë·ªÉ t·ªëi ƒëa h√≥a margin, t·ª©c l√† t·ªëi thi·ªÉu h√≥a $\frac{1}{2} \|w\|^2$, v·ªõi r√†ng bu·ªôc:")

    st.latex(r"\text{Minimize} \quad \frac{1}{2} \|w\|^2")

    st.latex(
        r"\text{Subject to} \quad y_i (w^T x_i + b) \geq 1 \quad \forall i")

    st.markdown(r"""

    #### 2.2. Tr∆∞·ªùng h·ª£p kh√¥ng tuy·∫øn t√≠nh ph√¢n t√°ch (Soft Margin SVM)

    Khi d·ªØ li·ªáu kh√¥ng th·ªÉ ph√¢n t√°ch tuy·∫øn t√≠nh ho√†n to√†n, SVM s·ª≠ d·ª•ng **Soft Margin** ƒë·ªÉ cho ph√©p m·ªôt s·ªë ƒëi·ªÉm b·ªã ph√¢n
    lo·∫°i sai ho·∫∑c n·∫±m trong margin. SVM c√°c bi·∫øn slack $\xi_i \geq 0$ v√† ƒëi·ªÅu ch·ªânh b√†i to√°n t·ªëi ∆∞u:""")

    st.latex(
        r"\text{Minimize} \quad \frac{1}{2} \|w\|^2 + C \sum_{i=1}^N \xi_i")

    st.latex(
        r"\text{Subject to} \quad y_i (w^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0 \quad \forall i")

    st.markdown(r"""

    Trong ƒë√≥ $C$ l√† tham s·ªë ƒëi·ªÅu ch·ªânh, c√¢n b·∫±ng gi·ªØa t·ªëi ƒëa h√≥a margin v√† gi·∫£m thi·ªÉu l·ªói ph√¢n lo·∫°i.
                
    $C$ l·ªõn: ∆Øu ti√™n gi·∫£m l·ªói ph√¢n lo·∫°i, d·∫´n ƒë·∫øn margin nh·ªè h∆°n (√≠t khoan dung v·ªõi l·ªói).
                
    $C$ nh·ªè: ∆Øu ti√™n t·ªëi ƒëa h√≥a margin, ch·∫•p nh·∫≠n nhi·ªÅu l·ªói h∆°n.
                
    **Bi·∫øn slack $\xi_i$** ƒëo l∆∞·ªùng m·ª©c ƒë·ªô vi ph·∫°m c·ªßa ƒëi·ªÉm d·ªØ li·ªáu $x_i$ ƒë·ªëi v·ªõi ƒëi·ªÅu ki·ªán margin:
    - $\xi_i = 0$: ƒêi·ªÉm $x_i$ ƒë∆∞·ª£c ph√¢n lo·∫°i ƒë√∫ng v√† n·∫±m ngo√†i margin (ho·∫∑c tr√™n ranh gi·ªõi margin).
    - $0 < \xi_i \leq 1 $: ƒêi·ªÉm $x_i$ n·∫±m trong margin nh∆∞ng v·∫´n ƒë∆∞·ª£c ph√¢n lo·∫°i ƒë√∫ng.
    - $\xi_i > 1$: ƒêi·ªÉm $x_i$ b·ªã ph√¢n lo·∫°i sai (n·∫±m ·ªü ph√≠a sai c·ªßa si√™u ph·∫≥ng ph√¢n t√°ch).

    Bi·∫øn slack gi√∫p SVM linh ho·∫°t h∆°n, cho ph√©p x·ª≠ l√Ω d·ªØ li·ªáu c√≥ nhi·ªÖu ho·∫∑c c√°c ƒëi·ªÉm ngo·∫°i lai, ƒë·ªìng th·ªùi c√¢n b·∫±ng gi·ªØa vi·ªác t·ªëi ƒëa h√≥a margin v√† gi·∫£m thi·ªÉu l·ªói ph√¢n lo·∫°i th√¥ng qua tham s·ªë $C$.

    #### 2.3. D·ªØ li·ªáu kh√¥ng tuy·∫øn t√≠nh (Kernel Trick)

    Khi d·ªØ li·ªáu kh√¥ng th·ªÉ ph√¢n t√°ch tuy·∫øn t√≠nh trong kh√¥ng gian ban ƒë·∫ßu, SVM s·ª≠ d·ª•ng **kernel trick** ƒë·ªÉ √°nh x·∫° d·ªØ li·ªáu l√™n kh√¥ng gian chi·ªÅu cao h∆°n. M·ªôt s·ªë kernel ph·ªï bi·∫øn:

    - **Linear Kernel**: $K(x_i, x_j) = x_i^T x_j$
    - **Polynomial Kernel**: $K(x_i, x_j) = (x_i^T x_j + 1)^d$
    - **RBF Kernel (Gaussian)**: $K(x_i, x_j) = \exp\left(-\gamma \|x_i - x_j\|^2\right)$

    B√†i to√°n t·ªëi ∆∞u ƒë∆∞·ª£c chuy·ªÉn sang d·∫°ng ƒë·ªëi ng·∫´u:""")

    st.latex(r"\text{Maximize} \quad \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j K(x_i, x_j)")

    st.latex(
        r"\text{Subject to} \quad \sum_{i=1}^N \alpha_i y_i = 0, \quad 0 \leq \alpha_i \leq C \quad \forall i")

    st.markdown(
        r"""Si√™u ph·∫≥ng ph√¢n t√°ch ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n c√°c support vectors (c√°c ƒëi·ªÉm c√≥ $\alpha_i > 0$)
    """)

    st.markdown(r"""
    ### 3. ∆Øu ƒëi·ªÉm c·ªßa SVM
    
    - Hi·ªáu qu·∫£ cao: SVM ho·∫°t ƒë·ªông r·∫•t t·ªët tr√™n c√°c b√†i to√°n ph√¢n lo·∫°i, ƒë·∫∑c bi·ªát khi d·ªØ li·ªáu c√≥ s·ªë chi·ªÅu l·ªõn (nh∆∞ MNIST).
    
    - Kh·∫£ nƒÉng t·ªïng qu√°t h√≥a t·ªët: Nh·ªù t·ªëi ƒëa h√≥a margin, SVM √≠t b·ªã overfitting (n·∫øu tham s·ªë C ƒë∆∞·ª£c ch·ªçn ph√π h·ª£p).
    
    - Linh ho·∫°t v·ªõi kernel: C√≥ th·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu kh√¥ng tuy·∫øn t√≠nh ph√¢n t√°ch b·∫±ng c√°ch s·ª≠ d·ª•ng kernel trick.
    
    - Ch·ªâ ph·ª• thu·ªôc v√†o support vectors: SVM kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi c√°c ƒëi·ªÉm d·ªØ li·ªáu xa si√™u ph·∫≥ng, gi√∫p gi·∫£m nh·∫°y c·∫£m v·ªõi nhi·ªÖu.
""")

    st.markdown(r"""
    ### 4. Nh∆∞·ª£c ƒëi·ªÉm c·ªßa SVM
                
    - T·ªën t√†i nguy√™n t√≠nh to√°n: SVM c√≥ ƒë·ªô ph·ª©c t·∫°p t√≠nh to√°n cao kh√¥ng ph√π h·ª£p v·ªõi t·∫≠p d·ªØ li·ªáu r·∫•t l·ªõn.
    
    - Nh·∫°y c·∫£m v·ªõi tham s·ªë: Hi·ªáu su·∫•t c·ªßa SVM ph·ª• thu·ªôc nhi·ªÅu v√†o vi·ªác ch·ªçn C, gamma, v√† lo·∫°i kernel.
    
    - Kh√≥ x·ª≠ l√Ω d·ªØ li·ªáu kh√¥ng chu·∫©n h√≥a: SVM nh·∫°y c·∫£m v·ªõi t·ª∑ l·ªá c·ªßa d·ªØ li·ªáu, c·∫ßn chu·∫©n h√≥a (scaling) tr∆∞·ªõc khi hu·∫•n luy·ªán.
""")


def full_theory():
    option = st.selectbox(
        'Ch·ªçn ph∆∞∆°ng ph√°p gi·∫£i th√≠ch:',
        ('DecisionTree', 'SVM')
    )

    if option == 'DecisionTree':
        decision_tree_theory()
    elif option == 'SVM':
        svm_theory()


def theory_info():
    st.title("Th√¥ng tin v·ªÅ c√°c thu·∫≠t to√°n")
    st.markdown("""
    - Decision Tree: Thu·∫≠t to√°n d·ª± ƒëo√°n gi√° tr·ªã ƒë·∫ßu ra d·ª±a tr√™n c√°c c√¢y quy·∫øt ƒë·ªãnh.
    - Support Vector Machine(SVM): Thu·∫≠t to√°n h·ªçc m√°y t√≠nh ƒë·∫∑c tr∆∞ng(SVM) cho ph√¢n l·ªõp hai ho·∫∑c nhi·ªÅu l·ªõp.
    """)
