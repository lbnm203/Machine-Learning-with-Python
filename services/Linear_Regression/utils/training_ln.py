import streamlit as st
import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import root_mean_squared_error, r2_score, accuracy_score, mean_squared_error
from sklearn.pipeline import Pipeline

from services.Linear_Regression.utils.preprocess import preprocess_data
import os
from mlflow.tracking import MlflowClient


def mlflow_input():
    st.title(" MLflow Tracking ")

    DAGSHUB_MLFLOW_URI = "https://dagshub.com/lbnm203/Machine_Learning_UI.mlflow"
    mlflow.set_tracking_uri(DAGSHUB_MLFLOW_URI)
    st.session_state['mlflow_url'] = DAGSHUB_MLFLOW_URI
    os.environ["MLFLOW_TRACKING_USERNAME"] = "lbnm203"
    os.environ["MLFLOW_TRACKING_PASSWORD"] = "0902d781e6c2b4adcd3cbf60e0f288a8085c5aab"

    mlflow.set_experiment("Linear_Regression")


def train_multiple_linear_regression(X_train, y_train, learning_rate=0.001, n_iterations=200):
    """Hu·∫•n luy·ªán h·ªìi quy tuy·∫øn t√≠nh b·ªôi b·∫±ng Gradient Descent."""

    # Chuy·ªÉn ƒë·ªïi X_train, y_train sang NumPy array ƒë·ªÉ tr√°nh l·ªói
    X_train = X_train.to_numpy() if isinstance(X_train, pd.DataFrame) else X_train
    y_train = y_train.to_numpy().reshape(-1, 1) if isinstance(y_train,
                                                              (pd.Series, pd.DataFrame)) else y_train.reshape(-1, 1)

    # Ki·ªÉm tra NaN ho·∫∑c Inf
    if np.isnan(X_train).any() or np.isnan(y_train).any():
        raise ValueError("D·ªØ li·ªáu ƒë·∫ßu v√†o ch·ª©a gi√° tr·ªã NaN!")
    if np.isinf(X_train).any() or np.isinf(y_train).any():
        raise ValueError("D·ªØ li·ªáu ƒë·∫ßu v√†o ch·ª©a gi√° tr·ªã v√¥ c√πng (Inf)!")

    # Chu·∫©n h√≥a d·ªØ li·ªáu ƒë·ªÉ tr√°nh tr√†n s·ªë
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)

    # L·∫•y s·ªë l∆∞·ª£ng m·∫´u (m) v√† s·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng (n)
    m, n = X_train.shape
    # st.write(f"S·ªë l∆∞·ª£ng m·∫´u (m): {m}, S·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng (n): {n}")

    # Th√™m c·ªôt bias (x0 = 1) v√†o X_train
    X_b = np.c_[np.ones((m, 1)), X_train]
    # st.write(f"K√≠ch th∆∞·ªõc ma tr·∫≠n X_b: {X_b.shape}")

    # Kh·ªüi t·∫°o tr·ªçng s·ªë ng·∫´u nhi√™n nh·ªè
    w = np.random.randn(X_b.shape[1], 1) * 0.01
    # st.write(f"Tr·ªçng s·ªë ban ƒë·∫ßu: {w.flatten()}")

    # Gradient Descent
    for iteration in range(n_iterations):
        gradients = (2/m) * X_b.T.dot(X_b.dot(w) - y_train)

        # Ki·ªÉm tra xem gradients c√≥ NaN kh√¥ng
        # st.write(gradients)
        if np.isnan(gradients).any():
            raise ValueError(
                "Gradient ch·ª©a gi√° tr·ªã NaN! H√£y ki·ªÉm tra l·∫°i d·ªØ li·ªáu ho·∫∑c learning rate.")

        w -= learning_rate * gradients

    # st.success("‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t!")
    # st.write(f"Tr·ªçng s·ªë cu·ªëi c√πng: {w.flatten()}")
    return w


def train_polynomial_regression(X_train, y_train, degree=2, learning_rate=0.001, n_iterations=500):
    """Hu·∫•n luy·ªán h·ªìi quy ƒëa th·ª©c **kh√¥ng c√≥ t∆∞∆°ng t√°c** b·∫±ng Gradient Descent."""

    # Chuy·ªÉn d·ªØ li·ªáu sang NumPy array n·∫øu l√† pandas DataFrame/Series
    X_train = X_train.to_numpy() if isinstance(X_train, pd.DataFrame) else X_train
    y_train = y_train.to_numpy().reshape(-1, 1) if isinstance(y_train,
                                                              (pd.Series, pd.DataFrame)) else y_train.reshape(-1, 1)

    # T·∫°o ƒë·∫∑c tr∆∞ng ƒëa th·ª©c **ch·ªâ th√™m b·∫≠c cao, kh√¥ng c√≥ t∆∞∆°ng t√°c**
    X_poly = np.hstack([X_train] + [X_train**d for d in range(2, degree + 1)])
    # Chu·∫©n h√≥a d·ªØ li·ªáu ƒë·ªÉ tr√°nh tr√†n s·ªë
    scaler = StandardScaler()
    X_poly = scaler.fit_transform(X_poly)

    # L·∫•y s·ªë l∆∞·ª£ng m·∫´u (m) v√† s·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng (n)
    m, n = X_poly.shape
    print(f"S·ªë l∆∞·ª£ng m·∫´u (m): {m}, S·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng (n): {n}")

    # Th√™m c·ªôt bias (x0 = 1)
    X_b = np.c_[np.ones((m, 1)), X_poly]
    print(f"K√≠ch th∆∞·ªõc ma tr·∫≠n X_b: {X_b.shape}")

    # Kh·ªüi t·∫°o tr·ªçng s·ªë ng·∫´u nhi√™n nh·ªè
    w = np.random.randn(X_b.shape[1], 1) * 0.01
    print(f"Tr·ªçng s·ªë ban ƒë·∫ßu: {w.flatten()}")

    # Gradient Descent
    for iteration in range(n_iterations):
        gradients = (2/m) * X_b.T.dot(X_b.dot(w) - y_train)

        # Ki·ªÉm tra n·∫øu gradient c√≥ gi√° tr·ªã NaN
        if np.isnan(gradients).any():
            raise ValueError(
                "Gradient ch·ª©a gi√° tr·ªã NaN! H√£y ki·ªÉm tra l·∫°i d·ªØ li·ªáu ho·∫∑c learning rate.")

        w -= learning_rate * gradients

    print("‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t!")
    print(f"Tr·ªçng s·ªë cu·ªëi c√πng: {w.flatten()}")

    return w


def training(data):
    st.write("## üìä Chia d·ªØ li·ªáu (Training - Validation - Testing)")

    # Ki·ªÉm tra n·∫øu `target_column` ch∆∞a t·ªìn t·∫°i
    # if "target_column" not in st.session_state or st.session_state.target_column not in data.columns:
    #     # M·∫∑c ƒë·ªãnh ch·ªçn c·ªôt ƒë·∫ßu ti√™n
    #     st.session_state.target_column = data.columns[0]

    if "target_column" not in st.session_state:
        st.session_state.target_column = data.columns[0]

    # Cho ph√©p ch·ªçn c·ªôt m·ª•c ti√™u nh∆∞ng kh√¥ng thay ƒë·ªïi session_state sau khi t·∫°o widget
    selected_label = st.selectbox("Ch·ªçn c·ªôt d·ª± ƒëo√°n", data.columns,
                                  index=data.columns.get_loc(st.session_state.target_column))

    X = data.drop(columns=[selected_label], axis=1)
    y = data[selected_label]

    # Ch·ªâ c·∫≠p nh·∫≠t session_state khi n√∫t ƒë∆∞·ª£c nh·∫•n
    if st.button("X√°c nh·∫≠n c·ªôt c·∫ßn d·ª± ƒëo√°n"):
        if st.session_state.target_column != selected_label:
            st.session_state.target_column = selected_label
            st.success(f"ƒê√£ ch·ªçn c·ªôt: **{selected_label}** l√†m bi·∫øn m·ª•c ti√™u")

    # Ki·ªÉm tra `st.session_state.data`
    if "data" in st.session_state:
        data = st.session_state.data
    else:
        st.error("H√£y t·∫£i t·∫≠p d·ªØ li·ªáu l√™n ƒë·ªÉ s·ª≠ d·ª•ng t√≠nh nƒÉng!")
        st.stop()

    test_size = st.slider("Ch·ªçn % testing", 10, 50, 20)
    val_size = st.slider("Ch·ªçn % validation", 0, 50, 15)
    remaining_size = 100 - test_size

    if st.button("Chia d·ªØ li·ªáu"):

        stratify_option = y if y.nunique() > 1 else None
        X_train_full, X_test, y_train_full, y_test = train_test_split(
            X, y, test_size=test_size/100, stratify=stratify_option, random_state=42
        )

        stratify_option = y_train_full if y_train_full.nunique() > 1 else None
        X_train, X_val, y_train, y_val = train_test_split(
            X_train_full, y_train_full, test_size=val_size / (100 - test_size),
            stratify=stratify_option, random_state=42
        )

        # L∆∞u v√†o session_state
        st.session_state.X_train = X_train
        st.session_state.X_test = X_test
        st.session_state.y_train = y_train
        st.session_state.y_test = y_test
        st.session_state.y = y
        st.session_state.X_val = X_val
        st.session_state.X_train_shape = X_train.shape[0]
        st.session_state.X_val_shape = X_val.shape[0]
        st.session_state.X_test_shape = X_test.shape[0]

    # Chia t·∫≠p Train - Validation b·∫±ng Cross Validation (KFold)
    k = st.slider("Ch·ªçn s·ªë k trong Cross Validation", 2, 10, 5)
    kf = KFold(n_splits=k, shuffle=True, random_state=42)

    st.write("### T·ª∑ l·ªá chia d·ªØ li·ªáu")
    if "X_train" in st.session_state and "X_val" in st.session_state and "X_test" in st.session_state:
        table_size = pd.DataFrame({
            'Dataset': ['Train', 'Validation', 'Test'],
            'K√≠ch th∆∞·ªõc (%)': [remaining_size - val_size, val_size, test_size],
            'S·ªë l∆∞·ª£ng m·∫´u': [st.session_state.X_train.shape[0], st.session_state.X_val.shape[0], st.session_state.X_test.shape[0]]
        })
        st.write(table_size)
        st.write(f" - S·ªë fold Cross Validation: {k}")
    else:
        st.warning("‚ö†Ô∏è Vui l√≤ng chia d·ªØ li·ªáu tr∆∞·ªõc khi hi·ªÉn th·ªã th√¥ng tin!")

    st.markdown("---")
    learning_rate = st.slider("Ch·ªçn t·ªëc ƒë·ªô h·ªçc (learning rate):",
                              min_value=1e-6, max_value=0.1, value=0.01, step=1e-6, format="%.6f")
    st.write("## ‚öôÔ∏è Hu·∫•n luy·ªán m√¥ h√¨nh")

    # model_option = st.selectbox(
    #     "Ch·ªçn m√¥ h√¨nh", ["Multiple Regression", "Polynomial Regression"])
    model_type_V = st.selectbox("Ch·ªçn lo·∫°i m√¥ h√¨nh:", [
        "Multiple Regression", "Polynomial Regression"])
    model_option = "linear" if model_type_V == "Multiple Regression" else "polynomial"

    degree = 2
    if model_option == "polynomial":
        degree = st.slider("Ch·ªçn b·∫≠c c·ªßa Polynomial Regression", 2, 5, 3)

    fold_mse = []
    scaler = StandardScaler()

    if "X_train" not in st.session_state or st.session_state.X_train is None:
        st.warning("‚ö†Ô∏è Vui l√≤ng chia d·ªØ li·ªáu tr∆∞·ªõc khi hu·∫•n luy·ªán m√¥ h√¨nh!")
        return None, None, None

    X_train, X_test = st.session_state.X_train, st.session_state.X_test
    y_train, y_test = st.session_state.y_train, st.session_state.y_test

    mlflow_input()

    st.session_state["run_name"] = f"{model_option}_run_default"
    if "run_counter" not in st.session_state:
        st.session_state["run_counter"] = 1
    else:
        st.session_state["run_counter"] += 1

    st.session_state["run_name"] = f"{model_option}_run_{st.session_state['run_counter']}"

    if st.button("Hu·∫•n luy·ªán m√¥ h√¨nh"):

        # if st.button("Hu·∫•n luy·ªán m√¥ h√¨nh"):
        # üéØ **T√≠ch h·ª£p MLflow**
        with mlflow.start_run(run_name=f"Train_{st.session_state['run_name']}"):
            data = st.session_state.data
            mlflow.log_param("dataset_shape", data.shape)
            mlflow.log_param("target_column", st.session_state.y.name)
            mlflow.log_param("test_size", st.session_state.X_test_shape)
            mlflow.log_param("validation_size",
                             st.session_state.X_val_shape)
            mlflow.log_param("train_size", st.session_state.X_train_shape)

            # L∆∞u dataset t·∫°m th·ªùi
            dataset_path = "dataset.csv"
            data.to_csv(dataset_path, index=False)

            # Log dataset l√™n MLflow
            mlflow.log_artifact(dataset_path)

            mlflow.log_param("model_option", model_option)
            mlflow.log_param("n_folds", k)
            mlflow.log_param("learning_rate", learning_rate)
            if model_option == "polynomial":
                mlflow.log_param("degree", degree)

            for fold, (train_idx, valid_idx) in enumerate(kf.split(X_train, y_train)):
                X_train_fold, X_valid = X_train.iloc[train_idx], X_train.iloc[valid_idx]
                y_train_fold, y_valid = y_train.iloc[train_idx], y_train.iloc[valid_idx]

                if model_option == "linear":
                    w = train_multiple_linear_regression(
                        X_train_fold, y_train_fold, learning_rate=learning_rate)
                    w = np.array(w).reshape(-1, 1)
                    X_valid_b = np.c_[
                        np.ones((len(X_valid), 1)), X_valid.to_numpy()]
                    y_valid_pred = X_valid_b.dot(w)
                else:
                    X_train_fold = scaler.fit_transform(X_train_fold)
                    w = train_polynomial_regression(
                        X_train_fold, y_train_fold, degree, learning_rate=learning_rate)
                    w = np.array(w).reshape(-1, 1)
                    X_valid_scaled = scaler.transform(X_valid.to_numpy())
                    X_valid_poly = np.hstack(
                        [X_valid_scaled] + [X_valid_scaled**d for d in range(2, degree + 1)])
                    X_valid_b = np.c_[
                        np.ones((len(X_valid_poly), 1)), X_valid_poly]
                    y_valid_pred = X_valid_b.dot(w)

                mse = mean_squared_error(y_valid, y_valid_pred)
                fold_mse.append(mse)
                mlflow.log_metric(f"mse_fold_{fold+1}", mse)
                print(f"üìå Fold {fold + 1} - MSE: {mse:.4f}")

            avg_mse = np.mean(fold_mse)

            if model_option == "linear":
                final_w = train_multiple_linear_regression(
                    X_train, y_train, learning_rate=learning_rate)
                st.session_state['linear_model'] = final_w
                X_test_b = np.c_[
                    np.ones((len(X_test), 1)), X_test.to_numpy()]
                y_test_pred = X_test_b.dot(final_w)
            else:
                X_train_scaled = scaler.fit_transform(X_train)
                final_w = train_polynomial_regression(
                    X_train_scaled, y_train, degree, learning_rate=learning_rate)
                st.session_state['polynomial_model'] = final_w
                X_test_scaled = scaler.transform(X_test.to_numpy())
                X_test_poly = np.hstack(
                    [X_test_scaled] + [X_test_scaled**d for d in range(2, degree + 1)])
                X_test_b = np.c_[
                    np.ones((len(X_test_poly), 1)), X_test_poly]
                y_test_pred = X_test_b.dot(final_w)

            test_mse = mean_squared_error(y_test, y_test_pred)

            # üìå **Log c√°c gi√° tr·ªã v√†o MLflow**
            mlflow.log_metric("avg_mse", avg_mse)
            mlflow.log_metric("test_mse", test_mse)

            # K·∫øt th√∫c run
            mlflow.end_run()

            st.success(f"MSE trung b√¨nh qua c√°c folds: {avg_mse:.4f}")
            st.success(f"MSE tr√™n t·∫≠p test: {test_mse:.4f}")
            st.success(
                f"Log d·ªØ li·ªáu **Train_{st.session_state['run_name']}_{model_option}** th√†nh c√¥ng!")
            # st.markdown(
            #     f"### üîó [Truy c·∫≠p MLflow m·ª•c Linear Regression ƒë·ªÉ xem tham s·ªë]({st.session_state['mlflow_url']})")

        return final_w, avg_mse, scaler
    return None, None, None
