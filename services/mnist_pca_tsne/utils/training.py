import streamlit as st
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split
import os
import mlflow
from mlflow.tracking import MlflowClient
import matplotlib.pyplot as plt
import numpy as np
import time
import plotly.express as px


def input_mlflow():
    DAGSHUB_MLFLOW_URI = "https://dagshub.com/lbnm203/Machine_Learning_UI.mlflow/"
    mlflow.set_tracking_uri(DAGSHUB_MLFLOW_URI)
    st.session_state['mlflow_url'] = DAGSHUB_MLFLOW_URI
    os.environ["MLFLOW_TRACKING_USERNAME"] = "lbnm203"
    os.environ["MLFLOW_TRACKING_PASSWORD"] = "0902d781e6c2b4adcd3cbf60e0f288a8085c5aab"
    mlflow.set_experiment("MNIST_PCA_t-SNE")


@st.cache_resource
def fit_tnse(X, n_components, perplexity, learning_rate, n_iter, metric):
    tsne = TSNE(n_components=n_components, perplexity=perplexity, learning_rate=learning_rate,
                n_iter=n_iter, metric=metric, random_state=42)
    X_train_tsne = tsne.fit_transform(X)
    return X_train_tsne, tsne


def train_pca(X, y):
    total_samples = X.shape[0]

    # Ch·ªçn s·ªë l∆∞·ª£ng ·∫£nh ƒë·ªÉ train
    num_samples = st.slider(
        'Ch·ªçn s·ªë l∆∞·ª£ng ·∫£nh cho ph·∫ßn hu·∫•n luy·ªán', 1000, total_samples, 10000, step=1000)

    X = X.reshape(X.shape[0], -1)
    y = y.reshape(-1)

    # Ch·ªçn s·ªë l∆∞·ª£ng ·∫£nh theo y√™u c·∫ßu
    X_selected, y_selected = X[:num_samples], y[:num_samples]

    st.write(f"- S·ªë l∆∞·ª£ng m·∫´u: {X_selected.shape[0]}")

    if "X_selected" not in st.session_state or "y_selected" not in st.session_state:
        st.session_state["X_selected"] = X_selected
        st.session_state["y_selected"] = y_selected

    input_mlflow()

    run_name = st.text_input("Nh·∫≠p t√™n Run:", "default")
    if not run_name:
        run_name = "default_run"
    st.session_state["run_name"] = run_name

    method = st.selectbox("Ch·ªçn ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu", ["PCA", "t-SNE"])
    n_components = st.slider("**S·ªë th√†nh ph·∫ßn ch√≠nh (n_components):**",
                             min_value=2, max_value=min(X_selected.shape[1], 3),
                             value=3,
                             help="""
S·ªë l∆∞·ª£ng chi·ªÅu (`n_components`) mu·ªën gi·ªØ l·∫°i sau khi gi·∫£m chi·ªÅu b·∫±ng PCA.
# """)

    if st.button("Ti·∫øn h√†nh gi·∫£m chi·ªÅu"):
        with st.spinner("ƒêang ti·∫øn h√†nh gi·∫£m chi·ªÅu v√† hi·ªÉn th·ªã bi·ªÉu ƒë·ªì..."):
            mlflow.start_run(run_name=st.session_state["run_name"])
            mlflow.log_param("method", method)
            mlflow.log_param("n_components", n_components)
            mlflow.log_param("num_samples", num_samples)
            mlflow.log_param("original_dim", X.shape[1])

            if method == "t-SNE":
                perplexity = min(30, num_samples - 1)
                mlflow.log_param("perplexity", perplexity)
                reducer = TSNE(n_components=n_components,
                               perplexity=perplexity, random_state=42)
            else:
                reducer = PCA(n_components=n_components)

            start_time = time.time()
            X_reduced = reducer.fit_transform(X_selected)
            elapsed_time = time.time() - start_time
            mlflow.log_metric("elapsed_time", elapsed_time)

            if method == "PCA":
                explained_variance = np.sum(reducer.explained_variance_ratio_)
                mlflow.log_metric("explained_variance_ratio",
                                  explained_variance)
            elif method == "t-SNE" and hasattr(reducer, "kl_divergence_"):
                mlflow.log_metric("KL_divergence", reducer.kl_divergence_)

            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            if n_components == 2:
                fig = px.scatter(x=X_reduced[:, 0], y=X_reduced[:, 1],
                                 color=y_selected.astype(str),
                                 title=f"{method} gi·∫£m chi·ªÅu xu·ªëng {n_components}D",
                                 labels={'x': "Th√†nh ph·∫ßn 1", 'y': "Th√†nh ph·∫ßn 2"})
            else:
                fig = px.scatter_3d(x=X_reduced[:, 0], y=X_reduced[:, 1], z=X_reduced[:, 2],
                                    color=y_selected.astype(str),
                                    title=f"{method} gi·∫£m chi·ªÅu xu·ªëng {n_components}D",
                                    labels={'x': "Th√†nh ph·∫ßn 1", 'y': "Th√†nh ph·∫ßn 2", 'z': "Th√†nh ph·∫ßn 3"})

            st.plotly_chart(fig)

            # L∆∞u k·∫øt qu·∫£ v√†o MLflow
            os.makedirs("logs", exist_ok=True)
            fig_path = f"logs/{method}_{n_components}D.png"
            fig.write_image(fig_path)
            mlflow.log_artifact(fig_path)

            mlflow.end_run()
            st.success(
                f"‚úÖ Log th√†nh c√¥ng d·ªØ li·ªáu **Train_{st.session_state['run_name']}**!")


#     dim_reduction_method = st.selectbox(
#         "**Ch·ªçn ph∆∞∆°ng ph√°p r√∫t g·ªçn chi·ªÅu d·ªØ li·ªáu:**", ["PCA", "t-SNE"])
#     if dim_reduction_method == "PCA":
#         col1, col2 = st.columns(2)
#         with col1:
#             # Tham s·ªë c·ªßa PCA
#             n_components = st.slider("**S·ªë th√†nh ph·∫ßn ch√≠nh (n_components):**",
#                                      min_value=2, max_value=min(X_selected.shape[1], 20),
#                                      value=5,
#                                      help="""
# S·ªë l∆∞·ª£ng chi·ªÅu (`n_components`) mu·ªën gi·ªØ l·∫°i sau khi gi·∫£m chi·ªÅu b·∫±ng PCA.
# - Gi√° tr·ªã nh·ªè h∆°n (v√≠ d·ª•: 2-5) ph√π h·ª£p cho tr·ª±c quan h√≥a, nh∆∞ng c√≥ th·ªÉ m·∫•t th√¥ng tin.
# - Gi√° tr·ªã l·ªõn h∆°n gi·ªØ l·∫°i nhi·ªÅu th√¥ng tin h∆°n nh∆∞ng l√†m tƒÉng ƒë·ªô ph·ª©c t·∫°p t√≠nh to√°n
# """)


#         if st.button("üöÄ Ch·∫°y PCA"):
#             progress_bar = st.progress(0)
#             for i in range(1, 101):
#                 progress_bar.progress(i)
#                 time.sleep(0.01)
#             st.write("Qu√° tr√¨nh hu·∫•n luy·ªán ƒë√£ ho√†n th√†nh!")
#             with mlflow.start_run(run_name=st.session_state["run_name"]) as run:
#                 mlflow.set_tag("mlflow.runName", st.session_state["run_name"])
#                 # √Åp d·ª•ng PCA
#                 pca = PCA(n_components=n_components,
#                           svd_solver=svd_solver, random_state=42)
#                 X_train_pca = pca.fit_transform(X_train)

#                 progress_bar = st.progress(0)
#                 for i in range(1, 101):
#                     progress_bar.progress(i)
#                     time.sleep(0.01)
#                 st.write("Qu√° tr√¨nh hu·∫•n luy·ªán ƒë√£ ho√†n th√†nh!")

#                 st.session_state.X_train_pca = X_train_pca
#                 st.session_state.explained_variance_ratio_ = pca.explained_variance_ratio_
#                 explained_variance = np.sum(pca.explained_variance_ratio_)

#                 # Log tham s·ªë v√†o MLflow
#                 mlflow.log_param("algorithm", dim_reduction_method)
#                 mlflow.log_param("n_components", n_components)
#                 mlflow.log_param("svd_solver", svd_solver)
#                 mlflow.log_param("X_train_pca", X_train_pca)
#                 mlflow.log_metric("explained_variance", explained_variance)

#                 # L∆∞u PCA data
#                 np.save("X_train_pca.npy", X_train_pca)
#                 mlflow.log_artifact("X_train_pca.npy")

#                 with col2:
#                     st.subheader(
#                         f"H√¨nh ·∫£nh k·∫øt qu·∫£: Gi·∫£m xu·ªëng c√≤n {n_components} chi·ªÅu d·ªØ li·ªáu s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p {dim_reduction_method}")
#                     fig2, ax = plt.subplots()
#                     scatter = ax.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train[:X_train_pca.shape[0]].astype(
#                         int), cmap='tab10', alpha=0.6)
#                     legend = ax.legend(
#                         *scatter.legend_elements(), title="Digits")
#                     ax.add_artist(legend)
#                     st.pyplot(fig2)
#                     fig2.savefig("pca_result.png")
#                     mlflow.log_artifact("pca_result.png")

#                 st.write("---")
#                 # Tr·ª±c quan h√≥a ph∆∞∆°ng sai gi·∫£i th√≠ch
#                 st.subheader(
#                     "K·∫øt qu·∫£ tr·ª±c quan h√≥a", help="""
# - Trong PCA:
#     - T·ª∑ l·ªá ph∆∞∆°ng sai gi·∫£i th√≠ch l√† ph·∫ßn trƒÉm ph∆∞∆°ng sai m√† m·ªói th√†nh ph·∫ßn ch√≠nh ƒë√≥ng g√≥p v√†o t·ªïng ph∆∞∆°ng sai
#     c·ªßa d·ªØ li·ªáu g·ªëc.
#         - √ù nghƒ©a: T·ª∑ l·ªá n√†y cho b·∫°n bi·∫øt m·ªói th√†nh ph·∫ßn ch√≠nh ƒë√≥ng g√≥p bao nhi√™u ph·∫ßn trƒÉm v√†o t·ªïng th√¥ng tin c·ªßa d·ªØ li·ªáu, gi√∫p d·ªÖ d√†ng ƒë√°nh gi√° xem
#         bao nhi√™u th√†nh ph·∫ßn c·∫ßn thi·∫øt ƒë·ªÉ gi·ªØ l·∫°i m·ªôt l∆∞·ª£ng th√¥ng tin nh·∫•t ƒë·ªãnh (v√≠ d·ª•: 90% ho·∫∑c 95%).
# """)

#             col1, col2 = st.columns([2, 1])
#             with col1:
#                 fig, ax = plt.subplots(1, 1, figsize=(6, 4))

#                 # Bi·ªÉu ƒë·ªì c·ªôt cho explained_variance_ratio_
#                 ax.bar(
#                     range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)
#                 ax.set_title("T·ª∑ l·ªá ph∆∞∆°ng sai gi·∫£i th√≠ch")
#                 ax.set_xlabel("Th√†nh ph·∫ßn ch√≠nh")
#                 ax.set_ylabel("T·ª∑ l·ªá")

#                 st.pyplot(fig)

#             with col2:
#                 explained_var_r = pca.explained_variance_ratio_
#                 explanation_data = {
#                     "Th√†nh ph·∫ßn ch√≠nh": [f"Th√†nh ph·∫ßn {i+1}" for i in range(len(explained_var_r))],
#                     "T·ª∑ l·ªá ph∆∞∆°ng sai gi·∫£i th√≠ch (%)": [f"{var*100:.2f}%" for var in explained_var_r]
#                 }
#                 st.table(explanation_data)

#             st.success(
#                 f"T·ªïng s·ªë ph∆∞∆°ng sai gi·∫£ th√≠ch: {sum(pca.explained_variance_ratio_)}")

#             st.success(
#                 f"Log tham s·ªë cho **Train_{st.session_state['run_name']}**!")
#             st.markdown(
#                 f"### üîó [Truy c·∫≠p MLflow DAGsHub]({st.session_state['mlflow_url']})")

#             mlflow.end_run()

#     elif dim_reduction_method == "t-SNE":
#         col1, col2 = st.columns(2)
#         with col1:
#             # Tham s·ªë c·ªßa t-SNE
#             n_components = st.selectbox("**S·ªë chi·ªÅu ƒë·∫ßu ra:**", [2, 3],
#                                         help="""S·ªë chi·ªÅu mu·ªën gi·∫£m xu·ªëng b·∫±ng t-SNE. Th∆∞·ªùng ch·ªâ c·∫ßn 2 chi·ªÅu ƒë·ªÉ
#                                         tr·ª±c quan h√≥a scatter plot (ph√π h·ª£p nh·∫•t v·ªõi MNIST). Gi√° tr·ªã 3 c√≥ th·ªÉ
#                                         h·ªØu √≠ch cho ph√¢n t√≠ch ph·ª©c t·∫°p h∆°n, nh∆∞ng tƒÉng th·ªùi gian t√≠nh to√°n v√†
#                                         kh√≥ tr·ª±c quan h√≥a h∆°n.""")
#             perplexity = st.slider(
#                 "**Perplexity:**", min_value=5, max_value=50, value=30, help="""
# Tham s·ªë perplexity ki·ªÉm so√°t s·ªë l∆∞·ª£ng ƒëi·ªÉm l√¢n c·∫≠n ƒë∆∞·ª£c xem x√©t trong qu√° tr√¨nh gi·∫£m chi·ªÅu.
# Gi√° tr·ªã nh·ªè (5-15) ph√π h·ª£p v·ªõi d·ªØ li·ªáu nh·ªè, gi√° tr·ªã l·ªõn (30-50) ph√π h·ª£p v·ªõi d·ªØ li·ªáu l·ªõn h∆°n.
# Gi√° tr·ªã qu√° l·ªõn ho·∫∑c qu√° nh·ªè c√≥ th·ªÉ l√†m m·∫•t c·∫•u tr√∫c d·ªØ li·ªáu.
# """)
#             learning_rate = st.slider(
#                 "**Learning rate:**", min_value=0.001, max_value=0.5, value=0.1, help="""
# T·ªëc ƒë·ªô h·ªçc (learning rate) ƒëi·ªÅu ch·ªânh b∆∞·ªõc di chuy·ªÉn c·ªßa thu·∫≠t to√°n t-SNE. Gi√° tr·ªã qu√° nh·ªè (0.001-0.01)
# c√≥ th·ªÉ l√†m ch·∫≠m qu√° tr√¨nh h·ªôi t·ª•, trong khi gi√° tr·ªã qu√° l·ªõn (0.1, 0.5) c√≥ th·ªÉ g√¢y m·∫•t ·ªïn ƒë·ªãnh. M·∫∑c ƒë·ªãnh
# 0.01 th∆∞·ªùng ph√π h·ª£p v·ªõi nhi·ªÅu d·ªØ li·ªáu.
# """)
#             n_iter = st.slider("**S·ªë v√≤ng l·∫∑p t·ªëi ƒëa:**",
#                                min_value=250, max_value=5000, value=1000, step=250, help="""
# S·ªë l·∫ßn l·∫∑p t·ªëi ƒëa cho t-SNE ƒë·ªÉ t·ªëi ∆∞u h√≥a. Gi√° tr·ªã nh·ªè (250-1000) gi·∫£m th·ªùi gian t√≠nh to√°n nh∆∞ng
# c√≥ th·ªÉ kh√¥ng h·ªôi t·ª• ƒë·∫ßy ƒë·ªß, trong khi gi√° tr·ªã l·ªõn (2000-5000) tƒÉng ƒë·ªô ch√≠nh x√°c nh∆∞ng k√©o d√†i th·ªùi gian.
# Ch·ªçn d·ª±a tr√™n k√≠ch th∆∞·ªõc d·ªØ li·ªáu v√† y√™u c·∫ßu t·ªëc ƒë·ªô.
# """)
#             metric = st.selectbox("**ƒê·ªô ƒëo kho·∫£ng c√°ch:**",
#                                   ["euclidean", "cosine", "manhattan"], help="""
# ƒê·ªô ƒëo kho·∫£ng c√°ch ƒë·ªÉ t√≠nh to√°n s·ª± t∆∞∆°ng ƒë·ªìng gi·ªØa c√°c ƒëi·ªÉm trong t-SNE:
# - `euclidean`: Kho·∫£ng c√°ch Euclid (m·∫∑c ƒë·ªãnh, ph√π h·ª£p cho d·ªØ li·ªáu s·ªë).
# - `cosine`: Kho·∫£ng c√°ch Cosine, h·ªØu √≠ch cho d·ªØ li·ªáu vector h∆∞·ªõng.
# - `manhattan`: Kho·∫£ng c√°ch Manhattan, ph√π h·ª£p v·ªõi d·ªØ li·ªáu c√≥ ph√¢n ph·ªëi ƒë·∫∑c bi·ªát.
# """)

#         if st.button("üöÄ Ch·∫°y t-SNE"):
#             progress_bar = st.progress(0)
#             for i in range(1, 101):
#                 progress_bar.progress(i)
#                 time.sleep(0.01)
#             st.write("Qu√° tr√¨nh hu·∫•n luy·ªán ƒë√£ ho√†n th√†nh!")
#             with mlflow.start_run(run_name=st.session_state["run_name"]) as run:
#                 mlflow.set_tag("mlflow.runName", st.session_state["run_name"])
#                 # # √Åp d·ª•ng t-SNE
#                 # tsne = TSNE(n_components=n_components, perplexity=perplexity, learning_rate=learning_rate,
#                 #             n_iter=n_iter, metric=metric, random_state=42)
#                 X_train_tsne, tsne = fit_tnse(
#                     X_train, n_components, perplexity, learning_rate, n_iter, metric)

#                 # L∆∞u k·∫øt qu·∫£ t-SNE
#                 st.session_state.X_train_tsne = X_train_tsne

#                 try:
#                     st.session_state.kl_divergence = tsne.kl_divergence_
#                 except AttributeError:
#                     st.session_state.kl_divergence = "Kh√¥ng c√≥ th√¥ng tin v·ªÅ k1 divergence"
#                 mlflow.log_param("algorithm", "t-SNE")
#                 mlflow.log_param("n_components", n_components)
#                 mlflow.log_param("perplexity", perplexity)
#                 mlflow.log_param("learning_rate", learning_rate)
#                 mlflow.log_param("n_iter", n_iter)
#                 mlflow.log_param("metric", metric)
#                 mlflow.log_param("X_train_tsne", X_train_tsne)

#                 np.save("X_train_tsne.npy", X_train_tsne)
#                 mlflow.log_artifact("X_train_tsne.npy")

#                 with col2:
#                     st.subheader(
#                         f"H√¨nh ·∫£nh k·∫øt qu·∫£: Gi·∫£m xu·ªëng c√≤n {n_components} chi·ªÅu d·ªØ li·ªáu s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p {dim_reduction_method}")
#                     fig2, ax = plt.subplots()
#                     scatter = ax.scatter(X_train_tsne[:, 0], X_train_tsne[:, 1], c=y_train[:X_train_tsne.shape[0]].astype(
#                         int), cmap='tab10', alpha=0.6)
#                     legend = ax.legend(
#                         *scatter.legend_elements(), title="Digits")
#                     ax.add_artist(legend)
#                     st.pyplot(fig2)
#                     fig2.savefig("tnse_result.png")
#                     mlflow.log_artifact("tnse_result.png")
#             st.success(
#                 f"Log tham s·ªë cho **Train_{st.session_state['run_name']}**!")
#             st.markdown(
#                 f"### üîó [Truy c·∫≠p MLflow DAGsHub]({st.session_state['mlflow_url']})")

#             mlflow.end_run()
